#!/usr/bin/env python3
import os
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"  # Match nvidia-smi GPU order

import torch
import time
import sys
import gc

source ./bin/activate

def get_free_memory(device):
    """Return free memory in bytes"""
    torch.cuda.synchronize(device)
    return torch.cuda.get_device_properties(device).total_memory - torch.cuda.memory_allocated(device)

def main():
    if len(sys.argv) != 2:
        print("Usage: python gpu-stress.py <gpu_id>")
        print("Example: python gpu-stress.py 1")
        sys.exit(1)

    try:
        gpu_id = int(sys.argv[1])
    except ValueError:
        print("Error: GPU ID must be an integer.")
        sys.exit(1)

    if not torch.cuda.is_available():
        print("CUDA is not available. Check driver and PyTorch installation.")
        sys.exit(1)

    if gpu_id >= torch.cuda.device_count():
        print(f"Error: GPU {gpu_id} does not exist. Found {torch.cuda.device_count()} GPUs.")
        sys.exit(1)

    device = torch.device(f'cuda:{gpu_id}')
    print(f'Using device: {torch.cuda.get_device_name(device)} (GPU {gpu_id})')

    # ALLOCATION BEGIN

    # === FRAGMENTATION-RESISTANT VRAM FILLER (GPU-Adaptive) ===
    total_mem = torch.cuda.get_device_properties(device).total_memory
    print(f'Total VRAM: {total_mem / 1e9:.2f} GB')

    # Adjust chunk size and max chunks based on GPU VRAM
    if total_mem >= 40 * 1e9:  # A100/H100
        chunk_elements = 128_000_000  # ~512 MB per tensor
        max_chunks = 60           # Up to ~30+ GB
        target_percent = 80
        print("High-end GPU detected (≥40GB) → using large chunks for deep VRAM fill")
    elif total_mem >= 20 * 1e9:  # 24GB class
        chunk_elements = 96_000_000
        max_chunks = 40
        target_percent = 80
        print("High VRAM GPU detected → moderate chunk scaling")
    else:  # 8–16GB GPUs (3070, etc.)
        chunk_elements = 64_000_000
        max_chunks = 30
        target_percent = 80
        print("Standard VRAM GPU detected → safe chunk sizing")

    target_bytes = int(target_percent / 100 * total_mem)
    print(f"Filling VRAM with small chunks (target: ~{target_percent}%) to allow compute headroom...")

    a_tensors = []
    b_tensors = []
    c_tensors = []
    allocated = torch.cuda.memory_allocated(device)
    print(f"Initial VRAM used: {allocated / 1e9:.2f} GB → Target: {target_bytes / 1e9:.2f} GB")

    for i in range(max_chunks):
        try:
            # Predict post-allocation memory
            new_allocation = 3 * chunk_elements * 4  # 3 tensors, float32
            if allocated + new_allocation > target_bytes:
                print(f"Next chunk would exceed {target_percent}% — stopping early.")
                break

            a_tensors.append(torch.randn(chunk_elements, device=device, dtype=torch.float32))
            b_tensors.append(torch.randn(chunk_elements, device=device, dtype=torch.float32))
            c_tensors.append(torch.randn(chunk_elements, device=device, dtype=torch.float32))

            torch.cuda.synchronize()
            allocated = torch.cuda.memory_allocated(device)
            print(f"Chunk {i+1}: VRAM used = {allocated / 1e9:.2f} GB")

        except torch.cuda.OutOfMemoryError:
            print(f"OOM at chunk {i+1} — stopping allocation.")
            # Roll back last (failed) allocation
            if len(a_tensors) > i:
                del a_tensors[-1]
            if len(b_tensors) > i:
                del b_tensors[-1]
            if len(c_tensors) > i:
                del c_tensors[-1]
            break

    if len(a_tensors) == 0:
        print("❌ Failed to allocate even one chunk. GPU memory unavailable.")
        sys.exit(1)

    # Set up compute tensors
    num_elements = a_tensors[0].numel()
    side = int((num_elements) ** 0.5)
    side = max(512, (side // 256) * 256)

    try:
        a = a_tensors[0][:side*side].view(side, side)
        b = b_tensors[0][:side*side].view(side, side)
        c = c_tensors[0][:side*side].view(side, side)
        torch.cuda.synchronize()
        d_test = torch.mm(a, b)
        torch.cuda.synchronize()
        del d_test
        print(f"Compute tensors ready: {side}x{side}")
    except Exception as e:
        print(f"Error setting up compute: {e}")
        sys.exit(1)

    allocated_final = torch.cuda.memory_allocated(device) / 1e9
    print(f"Final VRAM used: {allocated_final:.2f} GB → Ready for stress test.")

    # ALLOCATION END

    print(f"Final tensor size: {side}x{side}")
    print(f"Expected VRAM per tensor: {side * side * 4 / 1e9:.2f} GB")
    print(f"Expected total (3 tensors): {3 * side * side * 4 / 1e9:.2f} GB")

    # Warm up
    torch.mm(a, b)
    torch.relu(c)
    torch.cuda.synchronize()

    print(f'Starting 6-hour compute + memory stress test on GPU {gpu_id}...')
    start_time = time.time()
    duration = 21600  # 6 hours

    # Precompute transpose for memory access
    b_t = b.t().contiguous()

    try:
        while (time.time() - start_time) < duration:
            # Compute stress
            d = torch.mm(a, b)

            # Memory bandwidth stress:
            # - ReLU (activates all elements)
            # - Transpose and indexing (strided access)
            # - Random permutation (irregular memory access)
            # - Concatenation (memory movement)
            c = torch.relu(c + 0.001)
            c_slice = slice(0, side//2)
            c[c_slice, c_slice] += d[c_slice, c_slice]  # Partial write
            c = torch.cat([c[c.size(0)//2:], c[:c.size(0)//2]], dim=0)  # Shift rows
            if side >= 2:
                c = c + b_t[torch.randperm(b_t.size(0))][:c.size(0), :c.size(1)] # Random rows

            # Optional: force memory sync
            torch.cuda.synchronize()

            elapsed = int(time.time() - start_time)
            if elapsed % 10 == 0:
                allocated = torch.cuda.memory_allocated(device) / 1e9
                utilization = torch.cuda.utilization(device) if hasattr(torch.cuda, 'utilization') else 'N/A'
                print(f"GPU {gpu_id} | Time: {elapsed:5d}s/{duration}s | VRAM: {allocated:.2f} GB | Compute: {utilization}% (est) | Status: Active | Memory Ops: Active")

    except KeyboardInterrupt:
        print(f"\nGracefully shutting down stress test on GPU {gpu_id}...")
    except Exception as e:
        print(f"Error during stress test: {e}")
    finally:
        # Clean up
        if 'a' in locals(): del a
        if 'b' in locals(): del b
        if 'c' in locals(): del c
        if 'd' in locals(): del d
        if 'b_t' in locals(): del b_t
        torch.cuda.synchronize()
        torch.cuda.empty_cache()
        print(f"Stress test on GPU {gpu_id} completed. VRAM cleared.")

if __name__ == "__main__":
    main()
